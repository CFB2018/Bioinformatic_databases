{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d970da8-658c-4999-9784-ee6ab22cdae7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ENTREZ, import modules\n",
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77452885-ad6e-4f4b-975a-06aa593166f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package Bio.Entrez in Bio:\n",
      "\n",
      "NAME\n",
      "    Bio.Entrez - Provides code to access NCBI over the WWW.\n",
      "\n",
      "DESCRIPTION\n",
      "    The main Entrez web page is available at:\n",
      "    http://www.ncbi.nlm.nih.gov/Entrez/\n",
      "\n",
      "    Entrez Programming Utilities web page is available at:\n",
      "    http://www.ncbi.nlm.nih.gov/books/NBK25501/\n",
      "\n",
      "    This module provides a number of functions like ``efetch`` (short for\n",
      "    Entrez Fetch) which will return the data as a handle object. This is\n",
      "    a standard interface used in Python for reading data from a file, or\n",
      "    in this case a remote network connection, and provides methods like\n",
      "    ``.read()`` or offers iteration over the contents line by line. See\n",
      "    also \"What the heck is a handle?\" in the Biopython Tutorial and\n",
      "    Cookbook: http://biopython.org/DIST/docs/tutorial/Tutorial.html\n",
      "    http://biopython.org/DIST/docs/tutorial/Tutorial.pdf\n",
      "    The handle returned by these functions can be either in text mode or\n",
      "    in binary mode, depending on the data requested and the results\n",
      "    returned by NCBI Entrez. Typically, XML data will be in binary mode\n",
      "    while other data will be in text mode, as required by the downstream\n",
      "    parser to parse the data.\n",
      "\n",
      "    Unlike a handle to a file on disk from the ``open(filename)`` function,\n",
      "    which has a ``.name`` attribute giving the filename, the handles from\n",
      "    ``Bio.Entrez`` all have a ``.url`` attribute instead giving the URL\n",
      "    used to connect to the NCBI Entrez API.\n",
      "\n",
      "    All the functions that send requests to the NCBI Entrez API will\n",
      "    automatically respect the NCBI rate limit (of 3 requests per second\n",
      "    without an API key, or 10 requests per second with an API key) and\n",
      "    will automatically retry when encountering transient failures\n",
      "    (i.e. connection failures or HTTP 5XX codes). By default, Biopython\n",
      "    does a maximum of three tries before giving up, and sleeps for 15\n",
      "    seconds between tries. You can tweak these parameters by setting\n",
      "    ``Bio.Entrez.max_tries`` and ``Bio.Entrez.sleep_between_tries``.\n",
      "\n",
      "    The Entrez module also provides an XML parser which takes a handle\n",
      "    as input.\n",
      "\n",
      "    Variables:\n",
      "\n",
      "        - email        Set the Entrez email parameter (default is not set).\n",
      "        - tool         Set the Entrez tool parameter (default is ``biopython``).\n",
      "        - api_key      Personal API key from NCBI. If not set, only 3 queries per\n",
      "          second are allowed. 10 queries per seconds otherwise with a\n",
      "          valid API key.\n",
      "        - max_tries    Configures how many times failed requests will be\n",
      "          automatically retried on error (default is 3).\n",
      "        - sleep_between_tries   The delay, in seconds, before retrying a request on\n",
      "          error (default is 15).\n",
      "\n",
      "    Functions:\n",
      "\n",
      "        - efetch       Retrieves records in the requested format from a list of one or\n",
      "          more primary IDs or from the user's environment\n",
      "        - epost        Posts a file containing a list of primary IDs for future use in\n",
      "          the user's environment to use with subsequent search strategies\n",
      "        - esearch      Searches and retrieves primary IDs (for use in EFetch, ELink,\n",
      "          and ESummary) and term translations and optionally retains\n",
      "          results for future use in the user's environment.\n",
      "        - elink        Checks for the existence of an external or Related Articles link\n",
      "          from a list of one or more primary IDs.  Retrieves primary IDs\n",
      "          and relevancy scores for links to Entrez databases or Related\n",
      "          Articles;  creates a hyperlink to the primary LinkOut provider\n",
      "          for a specific ID and database, or lists LinkOut URLs\n",
      "          and Attributes for multiple IDs.\n",
      "        - einfo        Provides field index term counts, last update, and available\n",
      "          links for each database.\n",
      "        - esummary     Retrieves document summaries from a list of primary IDs or from\n",
      "          the user's environment.\n",
      "        - egquery      Provides Entrez database counts in XML for a single search\n",
      "          using Global Query.\n",
      "        - espell       Retrieves spelling suggestions.\n",
      "        - ecitmatch    Retrieves PubMed IDs (PMIDs) that correspond to a set of\n",
      "          input citation strings.\n",
      "\n",
      "        - read         Parses the XML results returned by any of the above functions.\n",
      "          Alternatively, the XML data can be read from a file opened in binary mode.\n",
      "          Typical usage is:\n",
      "\n",
      "              >>> from Bio import Entrez\n",
      "              >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "              >>> handle = Entrez.einfo() # or esearch, efetch, ...\n",
      "              >>> record = Entrez.read(handle)\n",
      "              >>> handle.close()\n",
      "\n",
      "           where record is now a Python dictionary or list.\n",
      "\n",
      "        - parse        Parses the XML results returned by those of the above functions\n",
      "          which can return multiple records - such as efetch, esummary\n",
      "          and elink. Typical usage is:\n",
      "\n",
      "              >>> handle = Entrez.esummary(db=\"pubmed\", id=\"19304878,14630660\", retmode=\"xml\")\n",
      "              >>> records = Entrez.parse(handle)\n",
      "              >>> for record in records:\n",
      "              ...     # each record is a Python dictionary or list.\n",
      "              ...     print(record['Title'])\n",
      "              Biopython: freely available Python tools for computational molecular biology and bioinformatics.\n",
      "              PDB file parser and structure class implemented in Python.\n",
      "              >>> handle.close()\n",
      "\n",
      "          This function is appropriate only if the XML file contains\n",
      "          multiple records, and is particular useful for large files.\n",
      "\n",
      "        - _open        Internally used function.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    Parser\n",
      "\n",
      "FUNCTIONS\n",
      "    ecitmatch(**keywds)\n",
      "        Retrieve PMIDs for input citation strings, returned as a handle.\n",
      "\n",
      "        ECitMatch retrieves PubMed IDs (PMIDs) that correspond to a set of input\n",
      "        citation strings.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ECitMatch\n",
      "\n",
      "        Return a handle to the results, by default in plain text\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        Short example:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> citation_1 = {\"journal_title\": \"proc natl acad sci u s a\",\n",
      "        ...               \"year\": \"1991\", \"volume\": \"88\", \"first_page\": \"3248\",\n",
      "        ...               \"author_name\": \"mann bj\", \"key\": \"citation_1\"}\n",
      "        >>> handle = Entrez.ecitmatch(db=\"pubmed\", bdata=[citation_1])\n",
      "        >>> print(handle.read().strip().split(\"|\"))\n",
      "        ['proc natl acad sci u s a', '1991', '88', '3248', 'mann bj', 'citation_1', '2014248']\n",
      "        >>> handle.close()\n",
      "\n",
      "    efetch(db, **keywords)\n",
      "        Fetch Entrez results which are returned as a handle.\n",
      "\n",
      "        EFetch retrieves records in the requested format from a list or set of one or\n",
      "        more UIs or from user's environment.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.EFetch\n",
      "\n",
      "        Return a handle to the results.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        Short example:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> handle = Entrez.efetch(db=\"nucleotide\", id=\"AY851612\", rettype=\"gb\", retmode=\"text\")\n",
      "        >>> print(handle.readline().strip())\n",
      "        LOCUS       AY851612                 892 bp    DNA     linear   PLN 10-APR-2007\n",
      "        >>> handle.close()\n",
      "\n",
      "        This will automatically use an HTTP POST rather than HTTP GET if there\n",
      "        are over 200 identifiers as recommended by the NCBI.\n",
      "\n",
      "        **Warning:** The NCBI changed the default retmode in Feb 2012, so many\n",
      "        databases which previously returned text output now give XML.\n",
      "\n",
      "    egquery(**keywds)\n",
      "        Provide Entrez database counts for a global search.\n",
      "\n",
      "        EGQuery provides Entrez database counts in XML for a single search\n",
      "        using Global Query.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.EGQuery\n",
      "\n",
      "        Return a handle to the results in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        This quick example based on a longer version from the Biopython\n",
      "        Tutorial just checks there are over 60 matches for 'Biopython'\n",
      "        in PubMedCentral:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> handle = Entrez.egquery(term=\"biopython\")\n",
      "        >>> record = Entrez.read(handle)\n",
      "        >>> handle.close()\n",
      "        >>> for row in record[\"eGQueryResult\"]:\n",
      "        ...     if \"pmc\" in row[\"DbName\"]:\n",
      "        ...         print(int(row[\"Count\"]) > 60)\n",
      "        True\n",
      "\n",
      "    einfo(**keywds)\n",
      "        Return a summary of the Entrez databases as a results handle.\n",
      "\n",
      "        EInfo provides field names, index term counts, last update, and\n",
      "        available links for each Entrez database.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.EInfo\n",
      "\n",
      "        Return a handle to the results, by default in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        Short example:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> record = Entrez.read(Entrez.einfo())\n",
      "        >>> 'pubmed' in record['DbList']\n",
      "        True\n",
      "\n",
      "    elink(**keywds)\n",
      "        Check for linked external articles and return a handle.\n",
      "\n",
      "        ELink checks for the existence of an external or Related Articles link\n",
      "        from a list of one or more primary IDs;  retrieves IDs and relevancy\n",
      "        scores for links to Entrez databases or Related Articles; creates a\n",
      "        hyperlink to the primary LinkOut provider for a specific ID and\n",
      "        database, or lists LinkOut URLs and attributes for multiple IDs.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ELink\n",
      "\n",
      "        Return a handle to the results, by default in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        This example finds articles related to the Biopython application\n",
      "        note's entry in the PubMed database:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> pmid = \"19304878\"\n",
      "        >>> handle = Entrez.elink(dbfrom=\"pubmed\", id=pmid, linkname=\"pubmed_pubmed\")\n",
      "        >>> record = Entrez.read(handle)\n",
      "        >>> handle.close()\n",
      "        >>> print(record[0][\"LinkSetDb\"][0][\"LinkName\"])\n",
      "        pubmed_pubmed\n",
      "        >>> linked = [link[\"Id\"] for link in record[0][\"LinkSetDb\"][0][\"Link\"]]\n",
      "        >>> \"17121776\" in linked\n",
      "        True\n",
      "\n",
      "        This is explained in much more detail in the Biopython Tutorial.\n",
      "\n",
      "    epost(db, **keywds)\n",
      "        Post a file of identifiers for future use.\n",
      "\n",
      "        Posts a file containing a list of UIs for future use in the user's\n",
      "        environment to use with subsequent search strategies.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.EPost\n",
      "\n",
      "        Return a handle to the results.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "    esearch(db, term, **keywds)\n",
      "        Run an Entrez search and return a handle to the results.\n",
      "\n",
      "        ESearch searches and retrieves primary IDs (for use in EFetch, ELink\n",
      "        and ESummary) and term translations, and optionally retains results\n",
      "        for future use in the user's environment.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
      "\n",
      "        Return a handle to the results which are always in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        Short example:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> handle = Entrez.esearch(db=\"nucleotide\", retmax=10, term=\"opuntia[ORGN] accD\", idtype=\"acc\")\n",
      "        >>> record = Entrez.read(handle)\n",
      "        >>> handle.close()\n",
      "        >>> int(record[\"Count\"]) >= 2\n",
      "        True\n",
      "        >>> \"EF590893.1\" in record[\"IdList\"]\n",
      "        True\n",
      "        >>> \"EF590892.1\" in record[\"IdList\"]\n",
      "        True\n",
      "\n",
      "    espell(**keywds)\n",
      "        Retrieve spelling suggestions as a results handle.\n",
      "\n",
      "        ESpell retrieves spelling suggestions, if available.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESpell\n",
      "\n",
      "        Return a handle to the results, by default in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        Short example:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> record = Entrez.read(Entrez.espell(term=\"biopythooon\"))\n",
      "        >>> print(record[\"Query\"])\n",
      "        biopythooon\n",
      "        >>> print(record[\"CorrectedQuery\"])\n",
      "        biopython\n",
      "\n",
      "    esummary(**keywds)\n",
      "        Retrieve document summaries as a results handle.\n",
      "\n",
      "        ESummary retrieves document summaries from a list of primary IDs or\n",
      "        from the user's environment.\n",
      "\n",
      "        See the online documentation for an explanation of the parameters:\n",
      "        http://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESummary\n",
      "\n",
      "        Return a handle to the results, by default in XML format.\n",
      "\n",
      "        Raises an IOError exception if there's a network error.\n",
      "\n",
      "        This example discovers more about entry 19923 in the structure\n",
      "        database:\n",
      "\n",
      "        >>> from Bio import Entrez\n",
      "        >>> Entrez.email = \"Your.Name.Here@example.org\"\n",
      "        >>> handle = Entrez.esummary(db=\"structure\", id=\"19923\")\n",
      "        >>> record = Entrez.read(handle)\n",
      "        >>> handle.close()\n",
      "        >>> print(record[0][\"Id\"])\n",
      "        19923\n",
      "        >>> print(record[0][\"PdbDescr\"])\n",
      "        Crystal Structure Of E. Coli Aconitase B\n",
      "\n",
      "    parse(handle, validate=True, escape=False)\n",
      "        Parse an XML file from the NCBI Entrez Utilities into python objects.\n",
      "\n",
      "        This function parses an XML file created by NCBI's Entrez Utilities,\n",
      "        returning a multilevel data structure of Python lists and dictionaries.\n",
      "        This function is suitable for XML files that (in Python) can be represented\n",
      "        as a list of individual records. Whereas 'read' reads the complete file\n",
      "        and returns a single Python list, 'parse' is a generator function that\n",
      "        returns the records one by one. This function is therefore particularly\n",
      "        useful for parsing large files.\n",
      "\n",
      "        Most XML files returned by NCBI's Entrez Utilities can be parsed by\n",
      "        this function, provided its DTD is available. Biopython includes the\n",
      "        DTDs for most commonly used Entrez Utilities.\n",
      "\n",
      "        The handle must be in binary mode. This allows the parser to detect the\n",
      "        encoding from the XML file, and to use it to convert all text in the XML\n",
      "        to the correct Unicode string. The functions in Bio.Entrez to access NCBI\n",
      "        Entrez will automatically return XML data in binary mode. For files,\n",
      "        please use mode \"rb\" when opening the file, as in\n",
      "\n",
      "            >>> from Bio import Entrez\n",
      "            >>> handle = open(\"Entrez/pubmed1.xml\", \"rb\")  # opened in binary mode\n",
      "            >>> records = Entrez.parse(handle)\n",
      "            >>> for record in records:\n",
      "            ...     print(record['MedlineCitation']['Article']['Journal']['Title'])\n",
      "            ...\n",
      "            Social justice (San Francisco, Calif.)\n",
      "            Biochimica et biophysica acta\n",
      "            >>> handle.close()\n",
      "\n",
      "        If validate is True (default), the parser will validate the XML file\n",
      "        against the DTD, and raise an error if the XML file contains tags that\n",
      "        are not represented in the DTD. If validate is False, the parser will\n",
      "        simply skip such tags.\n",
      "\n",
      "        If escape is True, all characters that are not valid HTML are replaced\n",
      "        by HTML escape characters to guarantee that the returned strings are\n",
      "        valid HTML fragments. For example, a less-than sign (<) is replaced by\n",
      "        &lt;. If escape is False (default), the string is returned as is.\n",
      "\n",
      "        Whereas the data structure seems to consist of generic Python lists,\n",
      "        dictionaries, strings, and so on, each of these is actually a class\n",
      "        derived from the base type. This allows us to store the attributes\n",
      "        (if any) of each element in a dictionary my_element.attributes, and\n",
      "        the tag name in my_element.tag.\n",
      "\n",
      "    read(handle, validate=True, escape=False)\n",
      "        Parse an XML file from the NCBI Entrez Utilities into python objects.\n",
      "\n",
      "        This function parses an XML file created by NCBI's Entrez Utilities,\n",
      "        returning a multilevel data structure of Python lists and dictionaries.\n",
      "        Most XML files returned by NCBI's Entrez Utilities can be parsed by\n",
      "        this function, provided its DTD is available. Biopython includes the\n",
      "        DTDs for most commonly used Entrez Utilities.\n",
      "\n",
      "        The handle must be in binary mode. This allows the parser to detect the\n",
      "        encoding from the XML file, and to use it to convert all text in the XML\n",
      "        to the correct Unicode string. The functions in Bio.Entrez to access NCBI\n",
      "        Entrez will automatically return XML data in binary mode. For files,\n",
      "        please use mode \"rb\" when opening the file, as in\n",
      "\n",
      "            >>> from Bio import Entrez\n",
      "            >>> handle = open(\"Entrez/esearch1.xml\", \"rb\")  # opened in binary mode\n",
      "            >>> record = Entrez.read(handle)\n",
      "            >>> print(record['QueryTranslation'])\n",
      "            biopython[All Fields]\n",
      "            >>> handle.close()\n",
      "\n",
      "        If validate is True (default), the parser will validate the XML file\n",
      "        against the DTD, and raise an error if the XML file contains tags that\n",
      "        are not represented in the DTD. If validate is False, the parser will\n",
      "        simply skip such tags.\n",
      "\n",
      "        If escape is True, all characters that are not valid HTML are replaced\n",
      "        by HTML escape characters to guarantee that the returned strings are\n",
      "        valid HTML fragments. For example, a less-than sign (<) is replaced by\n",
      "        &lt;. If escape is False (default), the string is returned as is.\n",
      "\n",
      "        Whereas the data structure seems to consist of generic Python lists,\n",
      "        dictionaries, strings, and so on, each of these is actually a class\n",
      "        derived from the base type. This allows us to store the attributes\n",
      "        (if any) of each element in a dictionary my_element.attributes, and\n",
      "        the tag name in my_element.tag.\n",
      "\n",
      "DATA\n",
      "    api_key = None\n",
      "    email = None\n",
      "    max_tries = 3\n",
      "    sleep_between_tries = 15\n",
      "    tool = 'biopython'\n",
      "\n",
      "FILE\n",
      "    c:\\users\\marbj610\\appdata\\local\\anaconda3\\lib\\site-packages\\bio\\entrez\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(Entrez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd6da6b-9cc2-4b80-bfa7-4bcb997dc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To access NCBI Entrez programming utilities, type:\n",
    "Entrez.email = \"datacyclopes@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35de3b68-82df-43c8-b8f5-3464588fdaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed', 'protein', 'nuccore', 'ipg', 'nucleotide', 'structure', 'genome', 'annotinfo', 'assembly', 'bioproject', 'biosample', 'blastdbinfo', 'books', 'cdd', 'clinvar', 'gap', 'gapplus', 'grasp', 'dbvar', 'gene', 'gds', 'geoprofiles', 'medgen', 'mesh', 'nlmcatalog', 'omim', 'orgtrack', 'pmc', 'popset', 'proteinclusters', 'pcassay', 'protfam', 'pccompound', 'pcsubstance', 'seqannot', 'snp', 'sra', 'taxonomy', 'biocollections', 'gtr']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle = Entrez.einfo()\n",
    "record = Entrez.read(handle)\n",
    "record[\"DbList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad3cf45b-6d89-432e-9b19-2eaadd975540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PubMed bibliographic record'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PUBMED\n",
    "handle = Entrez.einfo(db=\"pubmed\")\n",
    "record = Entrez.read(handle)\n",
    "record[\"DbInfo\"][\"Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8975ecc-f6bf-4b02-a946-1a20f006e4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'37733308'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record[\"DbInfo\"][\"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a584eca2-1808-4336-92ef-38d811543714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39264232', '39258649', '39246299', '39241843', '39237206', '39233507', '39221376', '39210956', '39211574', '39198641', '39196465', '39194422', '39182722', '39176945', '39175607', '39160074', '39158305', '39151232', '39148556', '39147947']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle = Entrez.esearch(db=\"pubmed\", term=\"cybernetics\")\n",
    "record = Entrez.read(handle)\n",
    "record[\"IdList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39e3744d-83b0-43a6-96b9-bf0f971e165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wheeler L', 'Worrell SE', 'Balzekas I', 'Bilderbeek J', 'Hermes D', 'Croarkin P', 'Messina S', 'Van Gompel J', 'Miller KJ', 'Kremen V', 'Worrell GA'] Case report: Bridging limbic network epilepsy with psychiatric, memory, and sleep comorbidities: case illustrations of reversible psychosis symptoms during continuous, high-frequency ANT-DBS. 2024 Frontiers in network physiology\n",
      "['Blay EA', 'Piqueira JRC'] The Emergence of Edgar Morin's Complex Thinking. 2024 Anais da Academia Brasileira de Ciencias\n"
     ]
    }
   ],
   "source": [
    "handle = Entrez.esummary(db=\"pubmed\", id=\"39175607, 39258649\")\n",
    "records = Entrez.parse(handle)\n",
    "\n",
    "for record in records:\n",
    "    print(record[\"AuthorList\"], record[\"Title\"], record[\"PubDate\"], record[\"FullJournalName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2925a0fd-26c9-46d7-842f-3d8463c8dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" ?>\\n<!DOCTYPE PubmedArticleSet PUBLIC \"-//NLM//DTD PubMedArticle, 1st January 2024//EN\" \"https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_240101.dtd\">\\n<PubmedArticleSet>\\n<PubmedArticle><MedlineCitation Status=\"Publisher\" Owner=\"NLM\"><PMID Version=\"1\">39147947</PMID><DateRevised><Year>2024</Year><Month>08</Month><Day>15</Day></DateRevised><Article PubModel=\"Print-Electronic\"><Journal><ISSN IssnType=\"Electronic\">1554-3528</ISSN><JournalIssue CitedMedium=\"Internet\"><PubDate><Year>2024</Year><Month>Aug</Month><Day>15</Day></PubDate></JournalIssue><Title>Behavior research methods</Title><ISOAbbreviation>Behav Res Methods</ISOAbbreviation></Journal><ArticleTitle>A tutorial on open-source large language models for behavioral science.</ArticleTitle><ELocationID EIdType=\"doi\" ValidYN=\"Y\">10.3758/s13428-024-02455-8</ELocationID><Abstract><AbstractText>Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git . Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.</AbstractText><CopyrightInformation>&#xa9; 2024. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN=\"Y\"><Author ValidYN=\"Y\"><LastName>Hussain</LastName><ForeName>Zak</ForeName><Initials>Z</Initials><AffiliationInfo><Affiliation>University of Basel, Basel, Switzerland. zakir.a.s.hussain@gmail.com.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Max Planck Institute for Human Development, Berlin, Germany. zakir.a.s.hussain@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN=\"Y\"><LastName>Binz</LastName><ForeName>Marcel</ForeName><Initials>M</Initials><AffiliationInfo><Affiliation>Max Planck Institute for Biological Cybernetics, T&#xfc;bingen, Germany.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Helmholtz Center for Computational Health, Neuherberg, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN=\"Y\"><LastName>Mata</LastName><ForeName>Rui</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>University of Basel, Basel, Switzerland.</Affiliation></AffiliationInfo></Author><Author ValidYN=\"Y\"><LastName>Wulff</LastName><ForeName>Dirk U</ForeName><Initials>DU</Initials><AffiliationInfo><Affiliation>University of Basel, Basel, Switzerland.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Max Planck Institute for Human Development, Berlin, Germany.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN=\"Y\"><Grant><GrantID>197315,204700</GrantID><Agency>Schweizerischer Nationalfonds zur F&#xf6;rderung der Wissenschaftlichen Forschung</Agency><Country/></Grant></GrantList><PublicationTypeList><PublicationType UI=\"D016428\">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType=\"Electronic\"><Year>2024</Year><Month>08</Month><Day>15</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>Behav Res Methods</MedlineTA><NlmUniqueID>101244316</NlmUniqueID><ISSNLinking>1554-351X</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><KeywordList Owner=\"NOTNLM\"><Keyword MajorTopicYN=\"N\">Behavioral science</Keyword><Keyword MajorTopicYN=\"N\">Hugging face</Keyword><Keyword MajorTopicYN=\"N\">Large language models</Keyword></KeywordList></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus=\"accepted\"><Year>2024</Year><Month>5</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus=\"medline\"><Year>2024</Year><Month>8</Month><Day>16</Day><Hour>1</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus=\"pubmed\"><Year>2024</Year><Month>8</Month><Day>16</Day><Hour>1</Hour><Minute>43</Minute></PubMedPubDate><PubMedPubDate PubStatus=\"entrez\"><Year>2024</Year><Month>8</Month><Day>15</Day><Hour>23</Hour><Minute>27</Minute></PubMedPubDate></History><PublicationStatus>aheadofprint</PublicationStatus><ArticleIdList><ArticleId IdType=\"pubmed\">39147947</ArticleId><ArticleId IdType=\"doi\">10.3758/s13428-024-02455-8</ArticleId><ArticleId IdType=\"pii\">10.3758/s13428-024-02455-8</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Abdurahman, A., Vu, H., Zou, W., Ungar, L., &amp; Bhatia, S. (2023). A deep learning approach to personality assessment: Generalizing across items and expanding the reach of survey-based research. Journal of Personality and Social Psychology, Advance online publication https://doi.org/10.1037/pspp0000480</Citation></Reference><Reference><Citation>Aeschbach, S., Mata, R., Wulff, D.U. (2024). Mapping the Mind With Free Associations: A Tutorial Using the R Package associator. PsyArXiv[SPACE] https://doi.org/10.31234/osf.io/ra87s</Citation></Reference><Reference><Citation>Aka, A., &amp; Bhatia, S. (2022). Machine learning models for predicting, understanding, and influencing health perception. Journal of the Association for Consumer Research, 7(2), 142&#x2013;153. https://doi.org/10.1086/718456</Citation></Reference><Reference><Citation>Ali, M., Fromm, M., Thellmann, K., Rutmann, R., L&#xfc;bbering, M., Leveling, J., ..., Flores-Herr, N. (2023). Tokenizer Choice For LLM Training: Negligible or Crucial? arXiv[SPACE] https://arxiv.org/abs/2310.08754</Citation></Reference><Reference><Citation>Alishahi, A., Chrupa&#x142;a, G., &amp; Linzen, T. (2019). Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop. Natural Language Engineering, 25(4), 543&#x2013;557. https://doi.org/10.1017/S135132491900024X</Citation></Reference><Reference><Citation>An, A., Qian, P., Wilcox, E., Levy, R. (2019). Representation of constituents in neural language models: Coordination phrase as a case study. arXiv preprint arXiv:1909.04625</Citation></Reference><Reference><Citation>Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., &amp; Wingate, D. (2023). Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), 337&#x2013;351. https://doi.org/10.1017/pan.2023.2</Citation></Reference><Reference><Citation>Bender, E.M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610&#x2013;623</Citation></Reference><Reference><Citation>Bhatia, S. (2023). Exploring the Sources of Variance in Risky Decision Making with Large Language Models. https://doi.org/10.31234/osf.io/3hrnc</Citation></Reference><Reference><Citation>Binz, M., &amp; Schulz, E. (2022). Modeling human exploration through resource-rational reinforcement learning. Advances in Neural Information Processing Systems, 35, 31755&#x2013;31768.</Citation></Reference><Reference><Citation>Binz, M., &amp; Schulz, E. (2023a). Turning large language models into cognitive models. arXiv, https://arxiv.org/abs/2306.03917</Citation></Reference><Reference><Citation>Binz, M., &amp; Schulz, E. (2023b). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120</Citation></Reference><Reference><Citation>Bockting, C. L., Van Dis, E. A. M., Van Rooij, R., Zuidema, W., &amp; Bollen, J. (2023). Living guidelines for generative AI: Why scientists must oversee its use. Nature, 622(7984), 693&#x2013;696. https://doi.org/10.1038/d41586-023-03266-1</Citation></Reference><Reference><Citation>Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., ..., Liang, P. (2023). The Foundation Model Transparency Index. arXiv, https://arxiv.org/abs/2310.12941</Citation></Reference><Reference><Citation>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., et al. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, 1877&#x2013;1901.</Citation></Reference><Reference><Citation>Cassani, G., Guenther, F., Attanasio, G., Bianchi, F., &amp; Marelli, M. (2023). Meaning Modulations and Stability in Large Language Models: An Analysis of BERT Embeddings for Psycholinguistic Research. PsyArXiv, https://doi.org/10.31234/osf.io/b45ys</Citation></Reference><Reference><Citation>Chae, Y., Davidson, T. (2023). Large language models for text classification: from zero-shot learning to fine-tuning. OSF[SPACE] https://osf.io/5t6xz/</Citation></Reference><Reference><Citation>Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in Neural Iformation Pocessing Systems, 30, 4299&#x2013;4307.</Citation></Reference><Reference><Citation>Coda-Forno, J., Binz, M., Akata, Z., Botvinick, M., Wang, J.X., &amp; Schulz, E. (2023). Meta-in-context learning in large language models, arXiv[SPACE] https://arxiv.org/abs/2305.12907</Citation></Reference><Reference><Citation>Crossley, S., Heintz, A., Choi, J. S., Batchelor, J., Karimi, M., &amp; Malatinszky, A. (2023). A large-scaled corpus for assessing text readability. Behavior Research Methods, 55(2), 491&#x2013;507.</Citation></Reference><Reference><Citation>Cutler, A., &amp; Condon, D. M. (2023). Deep lexical hypothesis: Identifying personality structure in natural language. Journal of Personality and Social Psychology, 125(1), 173&#x2013;197. https://doi.org/10.1037/pspp0000443</Citation></Reference><Reference><Citation>Demszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., &amp; Pennebaker, J. W. (2023). Using large language models in psychology. Nature Reviews Psychology, 2, 688&#x2013;701. https://doi.org/10.1038/s44159-023-00241-5</Citation></Reference><Reference><Citation>Devlin, J., Chang, M.W., Lee, K., Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv, https://arxiv.org/abs/1810.04805</Citation></Reference><Reference><Citation>Feng, S. F., Wang, S., Zarnescu, S., &amp; Wilson, R. C. (2021). The dynamics of explore-exploit decisions reveal a signal-to-noise mechanism for random exploration. Scientific Reports, 11(1), 3077. https://doi.org/10.1038/s41598-021-82530-8</Citation></Reference><Reference><Citation>Flesch, R. (1948). A new readability yardstick. Journal of Applied Psychology, 32(3), 221.</Citation></Reference><Reference><Citation>Frantar, E., Ashkboos, S., Hoefler, T., &amp; Alistarh, D. (2022). GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv, https://arxiv.org/abs/2210.17323</Citation></Reference><Reference><Citation>Frederick, S. (2005). Cognitive reflection and decision making. Journal of Economic Perspectives, 19(4), 25&#x2013;42. https://doi.org/10.1257/089533005775196732</Citation></Reference><Reference><Citation>Gershman, S. J. (2018). Deconstructing the human algorithms for exploration. Cognition, 173, 34&#x2013;42. https://doi.org/10.1016/j.cognition.2017.12.014</Citation></Reference><Reference><Citation>Gilardi, F., Alizadeh, M., &amp; Kubli, M. (2023). ChatGPT outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30), e2305016120. https://doi.org/10.1073/pnas.2305016120</Citation></Reference><Reference><Citation>Goldberg, L. R., Johnson, J. A., Eber, H. W., Hogan, R., Ashton, M. C., Cloninger, C. R., &amp; Gough, H. G. (2006). The International Personality Item Pool and the future of public-domain personality measures. Journal of Research in Personality, 40(1), 84&#x2013;96. https://doi.org/10.1016/j.jrp.2005.08.007</Citation></Reference><Reference><Citation>Graves, A. (2012). Supervised sequence labelling with recurrent neural networks. Springer.</Citation></Reference><Reference><Citation>G&#xfc;nther, F., Rinaldi, L., &amp; Marelli, M. (2019). Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. Perspectives on Psychological Science, 14(6), 1006&#x2013;1033.</Citation></Reference><Reference><Citation>Haigh, M. (2016). Has the standard Cognitive Reflection Test become a victim of its own success? Advances in Cognitive Psychology, 12(3), 145&#x2013;149. https://doi.org/10.5709/acp-0193-5</Citation></Reference><Reference><Citation>Hinton, G., Vinyals, O., &amp; Dean, J. (2015). Distilling the Knowledge in a Neural Network</Citation></Reference><Reference><Citation>Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ..., &amp; Chen, W. (2022). LoRA: Low-Rank Adaptation of Large Language Models. International Conference on Learning Representations, https://openreview.net/forum?id=nZeVKeeFYf9</Citation></Reference><Reference><Citation>Hussain, Z., Mata, R., &amp; Wulff, D.U. (2023). Novel embeddings improve the prediction of risk perception. PsyArXiv, https://doi.org/10.31234/osf.io/yrjfb</Citation></Reference><Reference><Citation>Irving, G., &amp; Askell, A. (2019). AI safety needs social scientists. Distill[SPACE] https://doi.org/10.23915/distill.00014</Citation></Reference><Reference><Citation>Jelinek, F., Mercer, R. L., Bahl, L. R., &amp; Baker, J. K. (1977). Perplexity-a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1), S63&#x2013;S63.</Citation></Reference><Reference><Citation>Kajonius, P. J., &amp; Johnson, J. A. (2019). Assessing the structure of the Five Factor Model of Personality (IPIP-NEO-120) in the public domain. Europe&#x2019;s Journal of Psychology, 15(2), 260&#x2013;275. https://doi.org/10.5964/ejop.v15i2.1671</Citation></Reference><Reference><Citation>Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., ..., &amp; Amodei, D., (2020). Scaling laws for neural language models. arXiv[SPACE] https://arxiv.org/abs/2001.08361</Citation></Reference><Reference><Citation>Kaufman, S., Rosset, S., Perlich, C., &amp; Stitelman, O. (2012). Leakage in data mining: Formulation, detection, and avoidance. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(4), 1&#x2013;21.</Citation></Reference><Reference><Citation>Korinek, A. (2023). Language Models and Cognitive Automation for Economic Research. NBER Working Paper Series, (30957) https://doi.org/10.3386/w30957</Citation></Reference><Reference><Citation>Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., &amp; Legg, S. (2018). Scalable agent alignment via reward modeling: a research direction. arXiv, https://arxiv.org/abs/1811.07871</Citation></Reference><Reference><Citation>Leike, J., &amp; Sutskever, I. (2023). Introducing Superalignment. OpenAI[SPACE] https://openai.com/blog/introducing-superalignment</Citation></Reference><Reference><Citation>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ..., Zettlemoyer, L. (2019). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv, https://arxiv.org/abs/1910.13461</Citation></Reference><Reference><Citation>Li, H. (2022). Language models: past, present, and future. Communications of the ACM, 65(7), 56&#x2013;63. https://doi.org/10.1145/3490443</Citation></Reference><Reference><Citation>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ..., &amp; Stoyanov, V. (2019). ROBERTa: A robustly optimized BERT pretraining approach. arXiv[SPACE] https://arxiv.org/abs/1907.11692</Citation></Reference><Reference><Citation>Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., ..., &amp; Wei, F. (2024). The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. arXiv preprint[SPACE] arXiv:2402.17764</Citation></Reference><Reference><Citation>Merkx, D., &amp; Frank, S.L. (2020). Human sentence processing: Recurrence or attention?. arXiv preprint[SPACE] arXiv:2005.09471</Citation></Reference><Reference><Citation>Mikolov, T., Chen, K., Corrado, G. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781</Citation></Reference><Reference><Citation>Mitchell, M. (2023). How do we know how smart AI systems are?. Science, 381(6654), eadj5957</Citation></Reference><Reference><Citation>Mitchell, M., &amp; Krakauer, D. C. (2023). The debate over understanding in AI&#x2019;s large language models. Proceedings of the National Academy of Sciences, 120(13), e2215907120.</Citation></Reference><Reference><Citation>Muennighoff, N., Tazi, N., Magne, L., &amp; Reimers, N. (2022). MTEB: Massive Text Embedding Benchmark. arXiv: https://arxiv.org/abs/2210.07316</Citation></Reference><Reference><Citation>OpenAI (2023). GPT-4 Technical Report. arXiv[SPACE] https://openai.com/research/gpt-4</Citation></Reference><Reference><Citation>Pelicon, A., Pranji&#x107;, M., Miljkovi&#x107;, D., &#x160;krlj, B., &amp; Pollak, S. (2020). Zero-shot learning for cross-lingual news sentiment classification. Applied Sciences, 10(17), 5993. https://doi.org/10.3390/app10175993</Citation></Reference><Reference><Citation>Prince, S.J. (2023). Understanding Deep Learning. MIT press</Citation></Reference><Reference><Citation>Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., &amp; Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36</Citation></Reference><Reference><Citation>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., &amp; Matena, ..., Liu, P.J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1), 5485&#x2013;5551. https://doi.org/10.5555/3455716.3455856</Citation></Reference><Reference><Citation>Rathje, S., Mirea, D.M., Sucholutsky, I., Marjieh, R., &amp; Robertson, C. (2023). GPT is an effective tool for multilingual psychological text analysis. PsyArXiv, https://osf.io/preprints/psyarxiv/sekf5/</Citation></Reference><Reference><Citation>Reimers, N., &amp; Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, https://arxiv.org/abs/1908.10084</Citation></Reference><Reference><Citation>Rosenbusch, H., Stevenson, C. E., &amp; Van Der Maas, H. L. J. (2023). How Accurate are GPT-3&#x2019;s Hypotheses About Social Science Phenomena? Digital Society, 2, 26. https://doi.org/10.1007/s44206-023-00054-2</Citation></Reference><Reference><Citation>Russell, S. (2019). Human compatible: Artificial intelligence and the problem of control. Penguin</Citation></Reference><Reference><Citation>Sanderson, G. (2019). Neural Networks https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</Citation></Reference><Reference><Citation>Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv[SPACE] https://arxiv.org/abs/1910.01108</Citation></Reference><Reference><Citation>Searle, J. R. (1980). Minds, brains, and programs. Behavioral and brain sciences, 3(3), 417&#x2013;424.</Citation></Reference><Reference><Citation>Siew, C. S., Wulff, D. U., Beckage, N. M., &amp; Kenett, Y. N. (2019). Cognitive network science: A review of research on cognition through the lens of network representations, processes, and dynamics. Complexity, 2019, 2108423. https://doi.org/10.1155/2019/2108423</Citation></Reference><Reference><Citation>Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., ..., &amp; Catanzaro, B. (2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model. arXiv[SPACE] https://arxiv.org/abs/2201.11990</Citation></Reference><Reference><Citation>Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702&#x2013;712.</Citation></Reference><Reference><Citation>Strubell, E., Ganesh, A., &amp; McCallum, A. (2019). Energy and policy considerations for deep learning in NLP. arXiv[SPACE] https://arxiv.org/abs/1906.02243</Citation></Reference><Reference><Citation>Su, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Ostendorf, M., ..., &amp; Yu, T. (2022). One embedder, any task: Instruction-finetuned text embeddings. arXiv[SPACE] https://arxiv.org/abs/2212.09741</Citation></Reference><Reference><Citation>Sucholutsky, I., Muttenthaler, L., Weller, A., Peng, A., Bobu, A., Kim, B., ..., &amp; Griffiths, T.L. (2023). Getting aligned on representational alignment. arXiv[SPACE] https://arxiv.org/abs/2310.13018</Citation></Reference><Reference><Citation>TheBloke (2022). Llama-2-7b-Chat-GPTQ. https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ</Citation></Reference><Reference><Citation>TII (2023). Falcon-40B-Instruct: A 40B parameters causal decoder-only model [Accessed: 2023-11-16] https://huggingface.co/tiiuae/falcon-40b-instruct</Citation></Reference><Reference><Citation>T&#xf6;rnberg, P. (2023). ChatGPT-4 outperforms experts and crowd workers in annotating political Twitter messages with zero-shot learning. arXiv[SPACE] https://arxiv.org/abs/2304.06588</Citation></Reference><Reference><Citation>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ..., &amp; Bhosale, S. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288</Citation></Reference><Reference><Citation>Tunstall, L., Von Werra, L., &amp; Wolf, T. (2022). Natural language processing with transformers. O&#x2019;Reilly.</Citation></Reference><Reference><Citation>Turing, A.M. (1950) I.-COMPUTING MACHINERY AND IN LIGENCE[_eprint: https://academic.oup.com/mind/article-pdf/LIX/236/433/30123314/lix-236-433.pdf ]. Mind, 59(236), 433&#x2013;460 https://doi.org/10.1093/mind/LIX.236.433</Citation></Reference><Reference><Citation>Van Noorden, R., &amp; Perkel, J. M. (2023). AI and science: what 1,600 researchers think. AI and science. Nature,621(7980), 672&#x2013;675. https://doi.org/10.1038/d41586-023-02980-0</Citation></Reference><Reference><Citation>Varma, S., &amp; Simon, R. (2006). Bias in error estimation when using cross-validation for model selection. BMC Bioinformatics,7(1), 1&#x2013;8. https://doi.org/10.1186/1471-2105-7-91</Citation></Reference><Reference><Citation>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp; Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30, 5998&#x2013;6008.</Citation></Reference><Reference><Citation>Vicente-Saez, R., &amp; Martinez-Fuentes, C. (2018). Open Science now: A systematic literature review for an integrated definition. Journal of Business Research, 88, 428&#x2013;436. https://doi.org/10.1016/j.jbusres.2017.12.043</Citation></Reference><Reference><Citation>Vig, J. (2019). A multiscale visualization of attention in the transformer model. arXiv[SPACE] https://arxiv.org/abs/1906.05714</Citation></Reference><Reference><Citation>Vig, J., &amp; Belinkov, Y. (2019). Analyzing the structure of attention in a transformer language model. arXiv[SPACE] https://arxiv.org/abs/1906.04284</Citation></Reference><Reference><Citation>Wang, T., Roberts, A., Hesslow, D., Le Scao, T., Chung, H.W., Beltagy, I., Launay, J., &amp; Raffel, C. (2022). What language model architecture and pretraining objective works best for zero-shot generalization? In: K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, &amp; S. Sabato (Eds.), Proceedings of the 39th International Conference on Machine Learning (pp. 22964&#x2013;22984) https://proceedings.mlr.press/v162/wang22u.html</Citation></Reference><Reference><Citation>Wang, Y., Huang, H., Rudin, C., &amp; Shaposhnik, Y. (2021). Understanding how dimension reduction tools work: an empirical approach to deciphering t-SNE, UMAP, TriMAP, and PaCMAP for data visualization. The Journal of Machine Learning Research, 22(1), 9129&#x2013;9201 https://dl.acm.org/doi/abs/10.5555/3546258.3546459</Citation></Reference><Reference><Citation>Webb, T., Holyoak, K. J., &amp; Lu, H. (2023). Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9), 1526&#x2013;1541. https://doi.org/10.1038/s41562-023-01659-w</Citation></Reference><Reference><Citation>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ..., &amp; Fedus, W. (2022). Emergent abilities of large language models. arXiv[SPACE] https://arxiv.org/abs/2206.07682</Citation></Reference><Reference><Citation>Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.S., Mellor, J., ..., &amp; Gabriel, I. (2022). Taxonomy of risks posed by language models. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency 214&#x2013;229 https://doi.org/10.1145/3531146.3533088</Citation></Reference><Reference><Citation>Wetzel, L. (2018). Types and Tokens. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Fall 2018). Metaphysics Research Lab: Stanford University.</Citation></Reference><Reference><Citation>Widmann, T., &amp; Wich, M. (2023). Creating and Comparing dictionary, word embedding, and transformer-based models to measure discrete emotions in German political text. Political Analysis, 31(4), 626&#x2013;641. https://doi.org/10.1017/pan.2022.15</Citation></Reference><Reference><Citation>Wilson, R. C., Geana, A., White, J. M., Ludvig, E. A., &amp; Cohen, J. D. (2014). Humans use directed and random exploration to solve the explore-exploit dilemma. Journal of Experimental Psychology: General, 143(6), 2074&#x2013;2081. https://doi.org/10.1037/a0038199</Citation></Reference><Reference><Citation>Wu, Y., Schuster, M, Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., ..., &amp; Dean, J. (2016). Google&#x2019;s neural machine translation system: Bridging the gap between human and machine translation. arXiv[SPACE] https://arxiv.org/abs/1609.08144</Citation></Reference><Reference><Citation>Wulff, D.U., &amp; Mata, R. (2023). Automated jingle&#x2013;jangle detection: Using embeddings to tackle taxonomic incommensurability. https://doi.org/10.31234/osf.io/9h7aw</Citation></Reference><Reference><Citation>Yax, N., Anll&#xf3;, H., Palminteri, &amp; Stefano, S. (2023). Studying and improving reasoning in humans and machines. arXiv[SPACE] https://arxiv.org/abs/2309.12485</Citation></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>'\n"
     ]
    }
   ],
   "source": [
    "handle = Entrez.efetch(db=\"pubmed\", id=\"39147947\")\n",
    "print(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b971b25-46e9-4122-ace2-ebc1add6ddf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
